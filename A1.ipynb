{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "A1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLRqn3y2NfJi"
      },
      "source": [
        "## CS431/631 Big Data Infrastructure\n",
        "### Winter 2022 - Assignment 1\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2dtUlLTNfJl"
      },
      "source": [
        "**Please edit this (text) cell to provide your name and UW student ID number!**\n",
        "* **Name:** Clement Wu\n",
        "* **ID:** 20775633"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3DCzT-rNfJm"
      },
      "source": [
        "---\n",
        "#### Overview\n",
        "For this assignment, you will be using Python to analyze the [pointwise mutual information (PMI)](http://en.wikipedia.org/wiki/Pointwise_mutual_information) of tokens in the text of Shakespeare's plays.    For this assignment, you will need the same text file (`Shakespeare.txt`) that you used for Assignment 0.   You will also need the Python tokenizer module, `simple_tokenize.py`.\n",
        "\n",
        "If two events $x$ and $y$ are independent, their PMI will be zero.   A positive PMI indicates that $x$ and $y$ are more likely to co-occur than they would be if they were independent.   Similarly, a negative PMI indicates that $x$ and $y$ are less likely to co-occur.   The PMI of events $x$ and $y$ is given by\n",
        "\\begin{equation*}\n",
        "PMI(x,y) = \\log\\frac{p(x,y)}{p(x)p(y)}\n",
        "\\end{equation*}\n",
        "where $p(x)$ and $p(y)$ are the probabilities of occurrence of events $x$ and $y$, and $p(x,y)$ is the probability of co-occurrence of $x$ and $y$.\n",
        "\n",
        "For this assignment, the \"events\" that we are interested in are occurrences of tokens on lines of text in the input file.   For example, one event\n",
        "might represent the occurence of the token \"fire\" a line of text, and another might represent the occurrence of the token \"peace\".   In that case, $p(fire)$ represents the probability that \"fire\" will occur on a line of text, and $p(fire,peace)$ represents the probability that *both* \"fire\" and \"peace\" will occur on the *same* line.   For the purposes of these PMI computations, it does not matter how many times a given token occures on a single line.   Either a line contains a particular token (at least once), or it does not.   For example, consider this line of text:\n",
        "\n",
        "> three three three, said thrice\n",
        "\n",
        "For this line, the following token-pair events have occurred:\n",
        "- (three, said)\n",
        "- (three, thrice)\n",
        "- (said, three)\n",
        "- (said, thrice)\n",
        "- (thrice, three)\n",
        "- (thrice, said)\n",
        "\n",
        "Note that we are not interested in \"reflexive\" pairs, such as (thrice,thrice).\n",
        "\n",
        "In addition to the probabilities of events, we will also be interested in the absolute *number* of occurences of particular events, e.g., the number of lines in which \"fire\" occurs.   We will use $n(x)$ to represent the these numbers.\n",
        "\n",
        "Your main task for this assignment is to write Python code to analyze the PMI of tokens from Shakespeare's plays.    Based this analysis, we want to be able to answer two types of queries:\n",
        "\n",
        "* Two-Token Queries: Given a pair of tokens, $x$ and $y$, report the number of lines on which that pair co-occurs ($n(x,y)$) as well as $PMI(x,y)$.\n",
        "* One-Token Queries: Given a single token, $x$, report the number of lines on which that token occurs ($n(x)$).   In addition, report the five tokens that have the largest PMI with respect to $x$ (and their PMIs).   That is, report the five $y$'s for which $PMI(x,y)$ is largest.\n",
        "\n",
        "To avoid reporting spurious results for the one-token queries, we are only interested in token pairs that co-occur a sufficient number of times.   Therefore, we will use a *threshold* parameter for one-token queries.   A one-token query should only report pairs of tokens that co-occur at least *threshold* times in the input.   For example, given the threshold 12, a one-token query for \"fire\" the should report the five tokens that have the largest PMI (with respect to \"fire\") among all tokens that co-occur with \"fire\" on at least 12 lines.   If there are fewer than five such tokens, report fewer than five.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBJ1H1i_ENGE"
      },
      "source": [
        "Run the next block to download the text file (`Shakespeare.txt`) and the Python tokenizer module (`simple_tokenize.py`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcXDTtuqENqF"
      },
      "source": [
        "!wget -q https://student.cs.uwaterloo.ca/~cs451/content/cs431/Shakespeare.txt\n",
        "!wget -q https://student.cs.uwaterloo.ca/~cs451/content/cs431/simple_tokenize.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVFO1bonNfJo"
      },
      "source": [
        "---\n",
        "#### Question 1  (2/10 marks):\n",
        "\n",
        "Before writing code to handle the PMI queries, start writing some code to answer some simpler questions that give an\n",
        "idea of how big the PMI analysis problem will be.   The box below contains some starter code that reads in the 'Shakespeare.txt' file and tokenizes it one line at time.   (This is the same code you started with for Assignment 0.)  Extend this code to determine (a) the number of *distinct* tokens that exist in 'Shakespeare.txt', and (b) the number of \n",
        "distinct token pairs that exist in 'Shakespeare.txt'.  For the purposes of this question, consider the token pair $x,y$ to be distinct from the pair $y,x$, i.e., count them both.   Ignore token pairs of the form $x,x$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "AWP7tAfsNfJp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f2332ec-7069-467b-f2a3-0b10a00914e0"
      },
      "source": [
        "# this imports the SimpleTokenize function from the simple_tokenize.py file that you uploaded\n",
        "from simple_tokenize import simple_tokenize\n",
        "\n",
        "# create single token list\n",
        "single = []\n",
        "\n",
        "#create pairs of token list\n",
        "pairs = []\n",
        "\n",
        "# Now, let's tokenize Shakespeare's plays\n",
        "with open('Shakespeare.txt') as f:\n",
        "    for line in f:\n",
        "        # tokenize, one line at a time\n",
        "        t = simple_tokenize(line)\n",
        "\n",
        "        # create temporary list of pairs of tokens in one line\n",
        "        tmp = ['( ' + i + ' , ' + j + ' )' for i in t for j in t if i!=j]\n",
        "        \n",
        "        # update single token list\n",
        "        single.extend(t)\n",
        "\n",
        "        # update token pairs list\n",
        "        pairs.extend(tmp)\n",
        "\n",
        "# close the file\n",
        "f.close()\n",
        "\n",
        "# take distinct items in both lists\n",
        "single = set(single)\n",
        "pairs = set(pairs)\n",
        "\n",
        "# number count of both lists\n",
        "print(\"The number of distinct tokens: \" + str(len(single)) + \" , \" + \"The number of distinct tokens: \"+ str(len(pairs)))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of distinct tokens: 25975 , The number of distinct tokens: 1969760\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpFi9CxkNfJq"
      },
      "source": [
        "---\n",
        "\n",
        "#### Question 2 (6/10 marks):\n",
        "Next, write Python code to answer the one-token and two-token queries described above, for 'Shakespeare.txt'.   The code cell below contains some starter code that implements a simple text-based query interface.  It allows a user to ask a series of one-token or two-token queries.   Try running the starter code to get a sense of how the interface behaves.    \n",
        "\n",
        "Your task is to write code to read and tokenize 'Shakespeare.txt', record information that will allow both types of PMI queries to be answered, and then answer queries that are posed through the query interface.  Make sure that your code is well commented, so that it will be clear to the markers.\n",
        "\n",
        "If you cannot get answers to both types of queries working, try to get at least one type working, for partial credit.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3of7ltFENfJr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb284219-1e3f-4903-d0ba-1f1b84d194e0"
      },
      "source": [
        "# this imports the SimpleTokenize function from the simple_tokenize.py file that you uploaded\n",
        "from simple_tokenize import simple_tokenize\n",
        "# the log function for computing PMI\n",
        "# for the sake of consistency across solutions, please use log base 10\n",
        "from math import log\n",
        "\n",
        "# the program gets the count of lines of all existing tokens and token pairs, also it calculates\n",
        "# the total number of lines\n",
        "def tokenCounter():\n",
        "  # create single token dictionary\n",
        "  single = {}\n",
        "\n",
        "  # create pair token dictionary\n",
        "  pair = {}\n",
        "\n",
        "  # line counter\n",
        "  lineNum = 0\n",
        "\n",
        "  # Now, let's tokenize Shakespeare's plays\n",
        "  with open('Shakespeare.txt') as f:\n",
        "      for line in f:\n",
        "          # update line number\n",
        "          lineNum += 1\n",
        "\n",
        "          # tokenize, one line at a time\n",
        "          t = simple_tokenize(line)\n",
        "\n",
        "          # create temporary list of pairs of tokens in one line\n",
        "          tmp = ['( ' + i + ' , ' + j + ' )' for i in t for j in t if i!=j]\n",
        "          \n",
        "          # update single token dictionary\n",
        "          for token in set(t):\n",
        "            if token in single.keys():\n",
        "              single[token] += 1\n",
        "            else:\n",
        "              single[token] = 1\n",
        "\n",
        "          # update token pairs dictionary\n",
        "          for pairToken in set(tmp):\n",
        "            if pairToken in pair.keys():\n",
        "              pair[pairToken] += 1\n",
        "            else:\n",
        "              pair[pairToken] = 1\n",
        "\n",
        "  # close the file\n",
        "  f.close()\n",
        "  \n",
        "  return single, pair, lineNum\n",
        "\n",
        "# the program inputs a token, a dictionary, and total number of lines. It outputs\n",
        "# the probility of token appearing in one line\n",
        "def probability(token, dictionary, lineNum):\n",
        "  return dictionary[token]/lineNum\n",
        "\n",
        "# the program calculates the pmi value by taking two tokens, a single token dictionary and pair token dictionary\n",
        "# and total number of lines\n",
        "def pmi(x, y, single, pair, lineNum):\n",
        "  return log(probability('( ' + x + ' , ' + y + ' )', pair, lineNum)/(probability(x, single, lineNum)*probability(y, single, lineNum)), 10)\n",
        "\n",
        "# the program generetes top five tokens that paired with an input token, which have the highest\n",
        "# pmi value\n",
        "def top5pmi(x, single, pair, threshold, lineNum):\n",
        "  # create y:pmi pairs and filter out items below threshold\n",
        "  filtered = {k.split(\" \")[3] : pmi(k.split(\" \")[1], k.split(\" \")[3], single, pair, lineNum) for k, v in pair.items() if v >= threshold and k.split(\" \")[1] == x}\n",
        "\n",
        "  # change dictionary to list of [y,pmi]\n",
        "  filtered_list = list(map(list, filtered.items()))\n",
        "\n",
        "  # sort list by pmi value\n",
        "  sort_list = sorted(filtered_list, key = lambda item: item[1], reverse = True)\n",
        "\n",
        "  # check of length is shorter than 5\n",
        "  if len(sort_list) < 5:\n",
        "    return sort_list\n",
        "  return sort_list[:5]\n",
        "\n",
        "# run the program to get all information\n",
        "single, pair, lineNum = tokenCounter()\n",
        "\n",
        "while True:\n",
        "    q = input(\"Input 1 or 2 space-separated tokens (return to quit): \")\n",
        "    if len(q) == 0:\n",
        "        break\n",
        "    q_tokens = simple_tokenize(q)\n",
        "\n",
        "    # one-token queries\n",
        "    if len(q_tokens) == 1:\n",
        "        threshold = 0\n",
        "        while threshold <= 0:\n",
        "            try:\n",
        "                threshold = int(input(\"Input a positive integer frequency threshold: \"))\n",
        "            except ValueError:\n",
        "                print(\"Threshold must be a positive integer!\")\n",
        "                continue\n",
        "        \n",
        "        # check if x exists in the file\n",
        "        if q_tokens[0] not in single.keys():\n",
        "          print(\"token does not exist!\")\n",
        "          continue\n",
        "\n",
        "        # get top 5 pmi value\n",
        "        top5 = top5pmi(q_tokens[0], single, pair, threshold, lineNum)\n",
        "\n",
        "        print(\"  n({0}) = {1}\".format(q_tokens[0], single[q_tokens[0]]))\n",
        "        print(\"  high PMI tokens with respect to {0} (threshold: {1}):\".format(q_tokens[0], threshold))\n",
        "        for i in range(len(top5)):\n",
        "          # print top 5 pmi values along with its x and y info\n",
        "          print(\"    n({0},{1}) = {2},  PMI({0},{1}) = {3}\".format(q_tokens[0], top5[i][0], pair['( ' + q_tokens[0] + ' , ' + top5[i][0] + ' )'], top5[i][1]))    \n",
        "\n",
        "    # two-token queries\n",
        "    elif len(q_tokens) == 2:\n",
        "        if q_tokens[0] not in single.keys() or q_tokens[1] not in single.keys():\n",
        "          print(\"at least one token does not exist!\")\n",
        "          continue\n",
        "\n",
        "        # print x, y and their mutual frequency\n",
        "        print(\"  n({0},{1}) = {2}\".format(q_tokens[0], q_tokens[1], pair['( ' + q_tokens[0] + ' , ' + q_tokens[1] + ' )']))\n",
        "\n",
        "        # print pmi value\n",
        "        print(\"  PMI({0},{1}) = {2}\".format(q_tokens[0], q_tokens[1], pmi(q_tokens[0],q_tokens[1], single, pair, lineNum)))\n",
        "    else:\n",
        "        print(\"Input must consist of 1 or 2 space-separated tokens!\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input 1 or 2 space-separated tokens (return to quit): man\n",
            "Input a positive integer frequency threshold: 20\n",
            "  n(man) = 1831\n",
            "  high PMI tokens with respect to man (threshold: 20):\n",
            "    n(man,honest) = 45,  PMI(man,honest) = 1.0282622233578977\n",
            "    n(man,old) = 89,  PMI(man,old) = 0.9302116488011463\n",
            "    n(man,every) = 56,  PMI(man,every) = 0.885957883693481\n",
            "    n(man,any) = 73,  PMI(man,any) = 0.847262207567516\n",
            "    n(man,woman) = 25,  PMI(man,woman) = 0.7060429286239783\n",
            "Input 1 or 2 space-separated tokens (return to quit): \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zjE_iW6NfJt"
      },
      "source": [
        "---\n",
        "\n",
        "#### Question 3 (2/10 marks):\n",
        "\n",
        "Suppose that you try to run your PMI analysis on larger files:  say, 10 times, or 100 times, or 1000 times larger than 'Shakespeare.txt'.    As the input file grows larger, what will happen to the execution of your program?   Will it get slower?   How much slower?   Will it eventually fail to run?   If so, why?\n",
        "\n",
        "In the cell below, briefly (one or two paragraphs), discuss what will happen if the input to your analysis grows.  We're not looking for an exact or empirical analysis of the behaviour of your program as a function of input size.  Rather, we are looking for a discussion of trends and limits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SVhvS1tNfJu"
      },
      "source": [
        "#### Answer to Question 3:\n",
        "\n",
        " My program has O(n^3) time complexity. If input file gets m times larger, the time required for the program to execute one iteration will gets m times larger, the time compexity of the program will grow m^3 higher. This means that my program will get slower and the time required to run it will grow exponentially. Since we are accumulating the stats of tokens, if ther file gets arebitrary large, it may cause memory overflow. This will make the program crash.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vsr1IWaDNfJu"
      },
      "source": [
        "---\n",
        "Don't forget to save your workbook!   (It's a good idea to do this regularly, while you are working.)   When you are finished and you are ready to submit your assignment, download your notebook file (.ipynb) from the hub to your machine, and then follow the submission instructions in the assignment."
      ]
    }
  ]
}